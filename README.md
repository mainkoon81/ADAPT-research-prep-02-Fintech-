# Intro to Fintech

## I. Robo-Advising
> Before Robo-Advisor
 - __For Rich:__ They would have investment portfolios and traditional advisors. So they'd be managing this portfolio of their money for themselves, for their family, for their retirement, and so on.
 - __For others:__ They wouldn't have financial advisors, or wouldn't even really be saving for their retirement so much. Instead, they depended on social security or pension plans such as "Defined Benefit Plan" and the pension was set by the formula, counting up the years of service.
   - Traditionally...**Insolvency Everywhere**...
     - The benefit came at a cost.
     - Employers should maintain assets sufficient for the liabilities..but do they do? GM? Chrysler? 
     - No guarantee of reliable delivery??? What are we going to do about the solvency of the pension plans? 
   - So these days, we were transitioning out of the DBP into "Defined Contribution Plans" - There was a promise there, such as "you're going to get this benefits!"
     - `401K Plan`
       - :The tax code encourages "Do-it-Yourself retirement savings".
       - :A sort of **"pre-tax"** (at the end of the month, money is taken out of my paycheck) goes into the mutual funds and then my employer also pitches in some money too. All that money goes into the mutual funds..then it grows...hopefully
       - :when I reach retirement, I've got my social security but on top of that, wherever this ends up, that's what I've got. That's what's standard.

### But How to help ordinary people for "Do it Yourself retirement savings"? How to help them come up with their own investment plan?
> Robo-Advisors:  
 - Taking the place of a human financial Advisors, it shows people the range of outcomes that they can expect! The real value of Robo_Advisor is that it actually better helps to achieve financial goals...
 - Robo-advisers deliver **high impact investment advice** at `high volume` and `low costs`!
   - Better handling the `logistics of your finances`
     - Aggregating your **accounts**
     - Displaying **financial situation**
     - Facilitating **transfers**
     - Directing your savings to the **right portfolio**!
> Portfolio Theory and Robo Advisor: 

**For a given expected return** you want to have the lowest risk. **For a given risk** you want to have the highest expected return. How to take **the least risk** for the expected return that you are targeting?
 - __Two Axioms of Economics:
   - 1) people prefer more to less
   - 2) people prefer more to less but they would get **decreasing amount of utility** out of the next dollar as the more wealthy they get.
     - So if you have a million bucks, the enjoyment you get out of another dollar is not as much as if you only had a hundred bucks...it means It's going up at a decreasing rate!
     <img src="https://user-images.githubusercontent.com/31917400/72269204-160a1d80-361b-11ea-934e-4c36169fd37b.jpg"/>
     
 - __But people are in general going to be risk averse:
   - Taking a pick: 
     - [A] getting 1000 bucks for sure ? 
     - [B] 0 or 2,000 bucks by gambling ?
   - People in general will choose A, but what if the expected return of A is going down, down.. and when to stop? You become ambivalent between:
     - [A] getting 970 bucks for sure ? Still Fine???
     - [B] 0 or 2,000 bucks by gambling ?
   - Reach the breaking point of 970: the `certainty equivalent` .. **still fine point**
     - which tells us how much risk averse you are. If it's 960 then you are much more risky averse..
     - which is the information that Robo-Advisor can use to customize your portfolio suggestion for you.
 - __Robo-Advisor do `mean-variance optimization`    







## II. Goal based Investment






## III. Application: Insur-Tech & RealEstate-Tech  
### A> InsurTechnology
<img src="https://user-images.githubusercontent.com/31917400/72616503-05b1b580-392f-11ea-8cd1-8ece2f44fa68.jpg"/>

Now, the ecosystem of technologies that are coming to play in the insurance industry are many:
 - __Blockchain:__ Efficient information exchange, trust, the ability to write contracts that are self-referential and self-aware and which offer a certain kind of immutability, ultimately appropriate because after all insurance is essentially in the form of a contract. 
 - __Analytics::__ Analytics that help insurers, their service providers, underwriters, and others make better decisions, taking more from data, expanding to new data, augmenting data. 
 - __Process Automation:__ The insurance industry and the processing of insurance requires vast amounts of repetitive tasks making insurance ripe for process automation. Rules-based or potentially ultimately self-aware, increasing reliability, decreasing mistakes, and ultimately automating what traditionally has been a human task oriented area. 
 - __Connecting ecosystems:__ As broad and far-reaching as social media or based on data already collected within the ecosystem of the insured linking back to the entire value chain of insurance, understanding the client better to better profile their risks and potentially spot both service lapses and rising risks in a more accurate way or in a faster way.
 - __drone technologies:__ As visual processing technologies, using for aerial imagery, remote assessment, and integrity of images and imagery used in the insurance underwriting process or for claims. 
 - __Artificial intelligence:__ AI, including natural language processing, chatbot, mimicking the human capacity to process language, to learn, to extract patterns not easily seen using traditional technologies including even advanced statistics and ultimately making a customer's experience and an insurers decision-making process better.
 - __Robo advisors:__ They rely on rules or other kinds of machine learning techniques to interact with customers, it could be customers interacting with technology online or over the phone either enhancing, making more immediate or `more accurate customer interactions`. 
 - __wearables:__ A very interesting area known as wearables, providing data both about the insured or those who are related to the insured all the way back to insurers helping them manage risks, understanding the insured, perhaps improving the customer experience in real-time or in aggregated way. 

#### Machine Learning ? 
The National Association of Insurance Commissioners indicates that only about 10 to 15 % of the data collected by insurance companies is currently actively used. Machine learning could allow those insurers to look at Big Data, to extract patterns that are useful for their businesses.
 - **Fraud managing**: Machine learning, including analyzing pictures and looking for certain markers of fraud, could both allow insurers to propagate their business models better, but also potentially decrease costs due to the dead weight loss of fraud.
 - **The automation of claims**:, which of course could lead to happier policy holders, is another area where machine learning and it's an efficiency and speed may apply, automating, reporting, processing, and speeding along the customer experience.
 - **risk modeling**:, allowing insurers to analyze their claims data in order to predict risk better. Of course, the historical data in the form of the history of losses may not be the only data that are important in understanding how to price risk and how to predict the risk...then Insurers could create models to predict demand for their own products and also develop new products, and therefore understand how to price them, or determine their premium.
 - **underwriting**: is one of the most important areas of insurance, Underwriters are those "human decision makers" who analyze data and, ultimately, make the decision of how to take on a given risk, and how to price it. Computers can aid that decision making process, they can flag risks in the process, and they can also point out inconsistencies in data that human underwriters may not be able to see. They can also check external sources, like social media, to verify accuracy of input data.

#### InsurTech Model Needed ?
 - 1.Product Design
 - 2.Selling and Marketing
 - 3.Underwrting(risk taking)
 - 4.Policy Administration(servicing clients and policies)
 - 5.Claim Management(paying out on insurance claims)
 <img src="https://user-images.githubusercontent.com/31917400/72667844-083c0a00-3a18-11ea-8596-359d0a633932.jpg"/>

#### ACTUARIAL MODELING in General
> Inverse Problem: Finding a cause of a consequence
 - result = K*f(cause) + error
 - If we find the K matrix, then it gets easy to find a cause.

Makov, Smith, and Liu (1996, p. 503) noted that "statistical methods with a Bayesian flavour, in particular credibility theory, have long been used in the insurance industry as part of the process of estimating risks and setting premiums."


 



------------------------------------------------------------------------------------------------------------
### B> RealEstateTechnology
Companies adopting the technology is bifurcated, either on the commercial side(CRE) or the non commercial side(NCRE). By such classification, there are very different players and very different characteristics. For example, **"Zillow"** as online portals and listing aggregators for residential real estate were on the scene back in the late-90s dot-com boom. It gives a democratized view into regions, properties, and other data for those who are buying or selling, so making people spend their time looking at their neighbors' properties...**"WeWork"** as a category leader on the commercial side, using app-based platform technology, is not just a provider of transitory real estate or real estate services but a provider of the technology of shared workspaces either rented or leased, changing social values and workplace concept.
 - >Residential Sector
   - This sector has long maintained a **pricing system** and **data availability** industrially.
     - MLS(Multiple Listing Services) has been in place for decades and has been run by local affiliates of NAR(National Association of Realtors). It was impractical to search for homes for sale without access to MLS data, which used to be mediated by realtors.
   - Today, the MLS is still owned at least in part by the NAR, but there has been pressure due to the advent of business models - Zillow, Trulia, Loopnet, Redfin, etc. Such that the MLS data has been made available, it's been democratized.
     - The historical commission structure of residential real estate sales (the “6% rule”) has been more resistant to change.
     - Technology is making it easier to disintermediate brokers, or force them to compete on price but residential real estate transactions usually require inspections, title services, evaluations, and an appraisal if financing is involved. 

 
 
 - >Commercial Sector
   - Managing commercial properties is often extremely complex and requires copious amounts of data and transaction details that are challenging to maintain(to obtain reliably). CRE Tech startups are focusing on this space and increasing efficiency, optimizing, and so on. For example,
     - Management information systems in the form of firms like **"Workframe"**. 
       - platforms for commercial tenants to gain information and to access contracts and to provide data and share data back and forth with commercial real estate managers. Transaction data aggregate are all examples of what's going on in the commercial real estate space.
     - WeSmartPark: Airbnb for empty garage parking spaces 
     - Bowery: Automating the CRE appraisal process 





------------------------------------------------------------------------------------------------------------

## IV. Inferring Causal Effects from Observational Data
### A> Causal Effects
Why we need Causality Analysis?
 - Fight againt "Spurious Correlation": you could have unrelated variables that might just coincidentally be highly correlated.. 
 <img src="https://user-images.githubusercontent.com/31917400/84554380-765ad380-ad0f-11ea-9ca5-7507a03974c6.jpg"/>

 - Make sure the relationship between two variables could be causal in either direction or what? Otherwise, we're stuck in the space of wondering which direction the causal arrow goes.
 <img src="https://user-images.githubusercontent.com/31917400/84554720-20872b00-ad11-11ea-87b1-d8dbf13b6f07.jpg"/>

Causal Effect
 - This causal effect where we're manipulating treatment on the same group of people versus this thing that we actually observe, which is the difference in means among some populations that are defined by treatment! In reality, for each person we're going to see one treatment, then we're going to see one outcome. But we want to infer something about what would have happened....we'll have to make assumptions to link observed data to potential outcomes. How do I estimate **causal effects from observational data**?
 <img src="https://user-images.githubusercontent.com/31917400/84555368-6db8cc00-ad14-11ea-9d6f-f3893d5aa44b.jpg"/>

Confounding Control? Killing some covariates?
 - Let's say we might be interested in the mean difference in the outcome if everybody was treated versus if no one was treated. To estimate this from observational data, we will need to make several assumptions, including ignorability. 
 - **Ignorability** refers to the `treatment assignment` **being "independent"(no relation to) of potential outcomes** conditional on some set of covariates X.
 <img src="https://user-images.githubusercontent.com/31917400/84569337-9f19b200-ad7d-11ea-8614-9fed2901eee6.jpg"/>

> Let's identify some "covariates X" making the "ignorability" assumption hold.(so...under the certain predictors, the treatment become obsolete in predicting the outcome!). What's the statistical method to control this?  
> Let's identify **Confounders** = Let's identify **multicollinearity**!

### B> Directed Acyclic Graphs


### C> Matching to kill Confounders
The randomization can kill the **confounders**. 
<img src="https://user-images.githubusercontent.com/31917400/84579891-cc8c4d00-adc9-11ea-96ff-4190813bbe1f.jpg"/>

Matching aims to achieve balance on observed covariates with the "imbalanced" outcome variable(categorical - treated/controlled). **Match individuals** in the treatment group (A=1) to people in the control group (A=0), but we'll match them on covariates X..In other words, for each treated person, we'll try to find a control person who has the same values of X. Find the best matches you can and then you **`get rid of the samples who weren't matched`**. And now you'll notice that we have perfect balance on this covariate. 
   - For example, in tha case where older people are more likely to get (A=1), and at younger ages, there are more people with (A=0). In a randomized trial via Matching, for any particular age, there should be about the same number of treated and untreated people. 
<img src="https://user-images.githubusercontent.com/31917400/84590486-06436f00-ae2f-11ea-89a2-037d8b7723fb.jpg"/>
 
If we had a single variable that we wanted to control for, and it was just a yes / no, binary kind of variable, it's easy but if you have many covariates, some of which might be continuous, it gets much more complicated.... Do Stochastic Balance!: the distribution of covariates will be balanced between the groups! It doesn't mean that we match exactly, but we'll have close matches and the distribution, then, of the covariates should be very similar in the two groups. 
   - Stochastic Balance says we can make the distribution of covariates in the control group look like that in the treated group.
 <img src="https://user-images.githubusercontent.com/31917400/84590732-faf14300-ae30-11ea-93e6-e627fc5fbcd0.jpg"/>

__C-a) Matching with Mahalanobis Distance__

> ### 1.[Preparation]
We can match directly on confounders....When we cannot match samples exactly, we first need to choose some metric of closeness...**Mahalanobis Distance**
<img src="https://user-images.githubusercontent.com/31917400/84592398-60e3c780-ae3d-11ea-918c-f1a294eab0f7.jpg"/> For example, if we have 3 covariates:
 - age
 - COPD(1:Yes, 0:No)
 - Female(1:Yes, 0:No)
 <img src="https://user-images.githubusercontent.com/31917400/84592618-f16ed780-ae3e-11ea-8572-bf5c30fce069.jpg"/> However, outliers sometimes could create a great distance between subjects, even if their covariates are otherwise similar....So an alternative is to use `ranks`. Just for the purpose of matching, we could replace all of our variables with their `ranks`. Make variables ordinal! 

> ### 2.[Matching via Nearest one]
Let's say you've already calculated the Mahalanobis distances b/w each **treated sample** - with every controlled samplesss. First, we randomly order list of treated samples and controlled samples. Starting with the first treated sample, match the controlled one with the smallest distance, then removing the matched controlled one from the list. Moving on to the next treated sample and repeat the matching process until you have matched all treated samples. 
 <img src="https://user-images.githubusercontent.com/31917400/84595227-8f1dd300-ae4e-11ea-8e93-bb683d885552.jpg"/>

> ### 3.[Matching via Optimal one]
When matching, it can look at the big picture - global minimum distance.   
 <img src="https://user-images.githubusercontent.com/31917400/84595643-ee7ce280-ae50-11ea-9960-ad8fc2ad6023.jpg"/>

> ### 4. Next, Check for balance b/w groups(covariates)! 
You can plot the `standardized mean differences` and this is especially useful if you have many covariants. It can show overall how well matching did. Is that matching created better balance on the covariates? 
 <img src="https://user-images.githubusercontent.com/31917400/84596016-18370900-ae53-11ea-9166-cba1b839259c.jpg"/>

> ### 5.[Outcome Analysis]
We might want to.. 
 - First choose and compute a test statistic from your observed data, and perform Hypothesis test with `H0: "There's no treatment effect"`.
 - Next, estimate the treatment effect and CI.... 

Randomization Test? Say that we are considering a binary classification problem and have a training set of m 'class-1' samples and n 'class-2' samples. A randomization test for `feature selection` looks at each feature individually. 
 - A test statistic θ, such as information gain or the **`normalized difference between the means`**, is calculated for the feature. 
 - The data for the feature is then randomly permuted and partitioned into two sets, one of size m and one of size n. 
 - The test statistic θ is then calculated again based on this new partition. 

Depending on the computational complexity of the problem, this is then repeated over all possible partitions of the feature into two sets of order m and n, or a random subset of these.

[Note] Random forests for `feature selection` usually uses the permutation approach: in order to compute the importance of a feature, it compare the decrease in accuracy after permutation. If you just delete that feature you are a bit less confident in comparing the resulting accuracy, the difference in RF accuracy might be higher because it takes into account that you helped the RF in decreasing its bias.

> ### Assuming it to be a binary classification problem, t-Statistics(comparing the "Group Means") helps us to evaluate that whether the values of a particular target variable for class(A=0) is significantly different from values of same target variable for class(A=1). If this holds, then the feature can helps us to better differentiate our data.

__C-b) Matching with Propensity Score__

Propensity score is simply the `probability of receiving treatment`, **given covariate X**. So..it's a sort of the population proportion `π` in accordance with a certain predictor. 
 - Propensity Score: ![formula](https://render.githubusercontent.com/render/math?math=\pi_i=P(A=1|X_i))
   <img src="https://user-images.githubusercontent.com/31917400/84644402-060ea680-aef7-11ea-831f-9fd88004184f.jpg"/>

 - __"propensity score is a balancing Score"?__ ![formula](https://render.githubusercontent.com/render/math?math=P(X=x|\pi(x)=score,A=1)=P(X=x|\pi(x)=score,A=0)) : A balancing score is something where if you condition on it, you'll have balance. Suppose two samples have the same value of the propensity Score, but have different predictor values. This means that those different predictor value is as likely to be found in the treatment group!...at the same rate! So...if we were to restrict to subpopulation of people that had the same value of the propensity score, then we should have balance in the two treatment groups. `so if matching on the propensity score, we can achieve the balance.` Ok, then how to estimate the propensity score for each sample? See `P(A=1|X)`...From ML model such as logistic regression(regressor + classifier), we can get predicted fitted value for each sample!   
 
 - __Propensity Score Matching:__ Once each propensity score is estimated, it is useful to look for "overlap". We can compare the distribution of the scores for the treated / controlled. <img src="https://user-images.githubusercontent.com/31917400/84662587-eb96f600-af13-11ea-8cb3-4a8187f5992b.jpg"/>
 
   - We hope that our **`positivity assumption`** is reasonable. The **positivity** refers to the situation where `all of the samples have at least some chance of receiving treatment`...so we hope the nice overlap in the plot. If there's a major lack of overlap in that at the high end of the propensity score, there's hardly anybody in the control group that had a propensity score like that. **So we really can't expect to learn about a treatment effect in the extremes**. We need to compare the group mean difference at the end.. we can't learn anything about a treatment effect among samples with no chance of getting treated.
   - **Randomization:** In this box above, these are a subpopulation who have covariates such that they really could have gotten treatment, and so treatment is effectively random within that range. So..get rid of individuals who have extreme propensity scores and focus on that box. This is what's known as trimming tails which means removing samples from your dataset that have extreme values of the propensity score (Remove any `control sample` whose propensity score is less than the minimum propensity score in the treatment group and chop off `treated samples` whose propensity score is greater than the maximum of the control group). 
   ### then now you could carry out matching after you trim the tails.








### D> Inverse Probability of Treatment Weighting





### E> Instrumental Variables Methods














